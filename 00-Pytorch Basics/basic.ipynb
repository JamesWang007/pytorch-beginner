{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch 基本处理单元"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-4.0790e-24,  4.5737e-41, -4.0790e-24,  4.5737e-41],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00,  1.8788e+31,  1.7220e+22],\n",
       "        [ 1.1970e+22,  1.0563e-05,  1.2857e-11,  1.6821e-04],\n",
       "        [ 2.1629e+23,  1.3666e+22,  5.3974e-05,  3.3242e+21]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 返回的数组大小5x4的矩阵\n",
    "torch.Tensor(5, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0136, 0.1146, 0.7065, 0.2566],\n",
       "        [0.4399, 0.5841, 0.3432, 0.0589],\n",
       "        [0.3141, 0.2398, 0.2793, 0.3053],\n",
       "        [0.7336, 0.0690, 0.5683, 0.1211],\n",
       "        [0.3972, 0.5709, 0.5106, 0.8231]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 返回的数组大小是5x4的矩阵，初始化是0~1的均匀分布\n",
    "torch.rand(5, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 4])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 得到矩阵大小\n",
    "a = torch.rand(5, 4)\n",
    "a.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# numpy 类似的返回5x4大小的矩阵\n",
    "np.ones((5, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.23093104 0.90491307 0.50040734 0.46130127]\n",
      " [0.6242984  0.21154219 0.11396891 0.00677162]\n",
      " [0.86012137 0.9592093  0.24215615 0.6900894 ]\n",
      " [0.30996364 0.812516   0.05599403 0.22562134]\n",
      " [0.44982022 0.45009744 0.01207632 0.4513023 ]]\n"
     ]
    }
   ],
   "source": [
    "# numpy 和 torch.Tensor 之间的转换\n",
    "a = torch.rand(5, 4)\n",
    "b = a.numpy()\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3, 4],\n",
      "        [3, 6]])\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[3, 4], [3, 6]])\n",
    "b = torch.from_numpy(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运算和numpy类似"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(5, 4)\n",
    "y = torch.rand(5, 4)\n",
    "c = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.9527, 0.6805, 1.9412, 2.6876],\n",
      "        [2.5958, 2.8978, 0.6307, 2.0800],\n",
      "        [0.1231, 2.5987, 0.8466, 0.8304],\n",
      "        [1.2684, 0.5182, 0.7438, 2.6416],\n",
      "        [2.1507, 1.9319, 0.3298, 2.7190]])\n"
     ]
    }
   ],
   "source": [
    "print(c * x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.8817, 0.2605, 1.6271, 1.2533],\n",
      "        [1.2475, 1.7266, 0.5682, 1.3952],\n",
      "        [0.6200, 1.6122, 0.4683, 1.2351],\n",
      "        [0.8005, 0.3558, 0.5066, 0.9087],\n",
      "        [1.6053, 0.7238, 0.1725, 1.2386]])\n"
     ]
    }
   ],
   "source": [
    "print(x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.8817, 0.2605, 1.6271, 1.2533],\n",
      "        [1.2475, 1.7266, 0.5682, 1.3952],\n",
      "        [0.6200, 1.6122, 0.4683, 1.2351],\n",
      "        [0.8005, 0.3558, 0.5066, 0.9087],\n",
      "        [1.6053, 0.7238, 0.1725, 1.2386]])\n"
     ]
    }
   ],
   "source": [
    "print(x.add(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.8817, 0.2605, 1.6271, 1.2533],\n",
       "        [1.2475, 1.7266, 0.5682, 1.3952],\n",
       "        [0.6200, 1.6122, 0.4683, 1.2351],\n",
       "        [0.8005, 0.3558, 0.5066, 0.9087],\n",
       "        [1.6053, 0.7238, 0.1725, 1.2386]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 可以直接进行操作改变原对象，x+y或者x.add()并不会改变x，但是x.add_()则会对x进行改变\n",
    "x.add_(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.8817, 0.2605, 1.6271, 1.2533],\n",
      "        [1.2475, 1.7266, 0.5682, 1.3952],\n",
      "        [0.6200, 1.6122, 0.4683, 1.2351],\n",
      "        [0.8005, 0.3558, 0.5066, 0.9087],\n",
      "        [1.6053, 0.7238, 0.1725, 1.2386]])\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将 torch.Tensor 放到 GPU 上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 判断一下电脑是否支持GPU\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7165, 0.4812, 0.2382, 0.9995],\n",
      "        [0.1362, 0.6725, 0.7586, 0.0124],\n",
      "        [0.1504, 0.3804, 0.1812, 0.4680],\n",
      "        [0.4265, 0.6717, 0.9109, 0.6045],\n",
      "        [0.2095, 0.6686, 0.9919, 0.7787]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(5, 4)\n",
    "a = a.cuda()\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch 的自动求导功能\n",
    "torch 和大部分框架一样有着自动求导功能，对象不再是 torch.Tensor，而是torch.autograd.Variable\n",
    "\n",
    "本质上Variable和Tensor没有什么区别，不过Variable会放在一个计算图里面，可以进行前向传播和反向传播以及求导  \n",
    "\n",
    "![1.png](http://upload-images.jianshu.io/upload_images/3623720-1c2694b72e0341ce.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n",
    "\n",
    "里面的creator表示通过什么操作得到的这个Variable，grad表示反向传播的梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# requires_grad 表示是否对其求梯度，默认是False\n",
    "x = Variable(torch.Tensor([3]), requires_grad=True)\n",
    "y = Variable(torch.Tensor([5]), requires_grad=True)\n",
    "z = 2 * x + y + 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对 x 和 y 分别求导\n",
    "z.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dz/dx: tensor([2.])\n",
      "dz/dy: tensor([1.])\n"
     ]
    }
   ],
   "source": [
    "# x 的导数和 y 的导数\n",
    "print('dz/dx: {}'.format(x.grad.data))\n",
    "print('dz/dy: {}'.format(y.grad.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 神经网络部分\n",
    "\n",
    "所依赖的主要是 torch.nn 和 torch.nn.functional\n",
    "\n",
    "torch.nn 里面有着所有的神经网络的层的操作，其用来构建网络，只有执行一次网络的运算才执行一次\n",
    "\n",
    "torch.nn.functional 表示的是直接对其做一次向前运算操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基本的网络构建类模板\n",
    "class net_name(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(net_name, self).__init__()\n",
    "        # 可以添加各种网络层\n",
    "        self.conv1 = nn.Conv2d(3, 10, 3)\n",
    "        # 具体每种层的参数可以去查看文档\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 定义向前传播\n",
    "        out = self.conv1(x)\n",
    "        return out"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
